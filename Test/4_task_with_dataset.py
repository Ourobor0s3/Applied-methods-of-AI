# 4_task_with_dataset.py

"""
–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ ClearML Dataset –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ.
–î–∞—Ç–∞—Å–µ—Ç –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω —Å —Å–µ—Ä–≤–µ—Ä–∞ ClearML –ø–æ –∏–º–µ–Ω–∏ –∏ –ø—Ä–æ–µ–∫—Ç—É,
—á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞–ø—É—Å–∫–∞—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∫–∞–∫ –ª–æ–∫–∞–ª—å–Ω–æ, —Ç–∞–∫ –∏ –Ω–∞ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–∞—Ö
–ë–ï–ó –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤—Ä—É—á–Ω—É—é —É–∫–∞–∑—ã–≤–∞—Ç—å Dataset ID.

–í–ê–ñ–ù–û: –ü–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º —Å–æ–∑–¥–∞–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç —Å –ø–æ–º–æ—â—å—é 3_dataset_creation.py
–∏–ª–∏ —á–µ—Ä–µ–∑ CLI: clearml-data create/add/close
"""

import os
from collections import Counter

import joblib
import matplotlib.pyplot as plt
import pandas as pd
import plotly.graph_objects as go
from clearml import Dataset, Task
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    auc,
    confusion_matrix,
    f1_score,
    precision_score,
    recall_score,
    roc_curve,
)
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures, StandardScaler

# üéØ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞ - —Ä–∞–±–æ—Ç–∞–µ–º –ø–æ –∏–º–µ–Ω–∏ –∏ –ø—Ä–æ–µ–∫—Ç—É!
# –≠—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—Å–ª–µ–¥–Ω—é—é –≤–µ—Ä—Å–∏—é –¥–∞—Ç–∞—Å–µ—Ç–∞
DATASET_PROJECT = "Tutorial"
DATASET_NAME = "Synthetic Dataset"

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ: –º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π ID –≤–µ—Ä—Å–∏–∏ –¥–ª—è –ø–æ–ª–Ω–æ–π –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
# –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π –∏ —É–∫–∞–∂–∏ ID, –µ—Å–ª–∏ –Ω—É–∂–Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –≤–µ—Ä—Å–∏—è
DATASET_ID = "your_dataset_id_here"
USE_SPECIFIC_VERSION = False  # –£—Å—Ç–∞–Ω–æ–≤–∏ True, —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å DATASET_ID

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–¥–∞—á—É –≤ ClearML
task = Task.init(
    project_name="Tutorial",
    task_name="Task with ClearML Dataset",
    output_uri=True,
)

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥–∏ –∫ –∑–∞–¥–∞—á–µ
task.add_tags(["polynomial-regression", "tutorial", "with-dataset"])

# –ü–æ–ª—É—á–∞–µ–º –ª–æ–≥–≥–µ—Ä
logger = task.get_logger()

print("=" * 60)
print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ ClearML")
print("=" * 60)

###########################
##### Dataset loading #####
###########################

# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å —Å–µ—Ä–≤–µ—Ä–∞ ClearML
# –î–∞—Ç–∞—Å–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫—ç—à–∏—Ä—É–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–æ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

try:
    if USE_SPECIFIC_VERSION and "DATASET_ID" in globals():
        # –°–ø–æ—Å–æ–± 1: –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –ø–æ ID
        print(f"\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ ID: {DATASET_ID}")
        dataset = Dataset.get(dataset_id=DATASET_ID)
    else:
        # –°–ø–æ—Å–æ–± 2: –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–∏ –ø–æ –∏–º–µ–Ω–∏ –∏ –ø—Ä–æ–µ–∫—Ç—É (–†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø)
        print(
            f"\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ –∏–º–µ–Ω–∏: {DATASET_PROJECT}/{DATASET_NAME}"
        )
        print("   (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è)")
        dataset = Dataset.get(
            dataset_project=DATASET_PROJECT,
            dataset_name=DATASET_NAME,
        )

    print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {dataset.name}")
    print(f"   ID –≤–µ—Ä—Å–∏–∏: {dataset.id}")
    print(f"   –ü—Ä–æ–µ–∫—Ç: {dataset.project}")

except Exception as e:
    print(f"\n‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
    print("\nüí° –£–±–µ–¥–∏—Å—å, —á—Ç–æ –¥–∞—Ç–∞—Å–µ—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç:")
    print("   1. –ó–∞–ø—É—Å—Ç–∏: python 3_dataset_creation.py")
    print("   2. –ò–ª–∏ —á–µ—Ä–µ–∑ CLI:")
    print(
        f'      clearml-data create --project {DATASET_PROJECT} --name "{DATASET_NAME}"'
    )
    print("      clearml-data add --files ./data/synthetic_dataset.csv")
    print("      clearml-data close")
    print("\n   3. –ü—Ä–æ–≤–µ—Ä—å —Å–ø–∏—Å–æ–∫ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤:")
    print(f"      clearml-data list --project {DATASET_PROJECT}")
    raise

# –ü–æ–ª—É—á–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
# get_local_copy() —Å–∫–∞—á–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ –Ω–µ–º—É
dataset_path = dataset.get_local_copy()
print(f"   –õ–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å: {dataset_path}")

# –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
files = dataset.list_files()
print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤: {len(files)}")
print(f"   –§–∞–π–ª—ã: {files}")

# –ó–∞–≥—Ä—É–∂–∞–µ–º CSV —Ñ–∞–π–ª
# –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ –æ–¥–∏–Ω CSV —Ñ–∞–π–ª
csv_file = [f for f in files if f.endswith(".csv")][0]
df = pd.read_csv(os.path.join(dataset_path, csv_file))

print("\n‚úÖ –î–∞—Ç–∞—Å–µ—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
print("=" * 60)

#####################
######## EDA ########
#####################

# –í—ã–≤–æ–¥–∏–º –æ–±—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
print("\nEDA: –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ")
print(df.info())

# –õ–æ–≥–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
dataset_info = {
    "Dataset shape": str(df.shape),
    "Number of features": str(df.shape[1] - 1),
    "Number of samples": str(df.shape[0]),
    "Target variable": "target",
    "ClearML Dataset ID": dataset.id,
    "Dataset Version": dataset.name,
}
info_df = pd.DataFrame(
    list(dataset_info.items()), columns=["Property", "Value"]
)
logger.report_table(
    title="Dataset Statistics",
    series="Basic Info",
    iteration=0,
    table_plot=info_df,
)

# –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º
print("EDA: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º")
stats_df = df.describe()
print(stats_df)
logger.report_table(
    title="Dataset Statistics",
    series="Numerical Features",
    iteration=0,
    table_plot=stats_df,
)

# –†–∞–∑–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ (X) –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (y)
X = df.drop("target", axis=1)
y = df["target"]

# –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {X_train.shape}")
print(f"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {X_test.shape}")

# –°–æ–∑–¥–∞–µ–º –∏ –ª–æ–≥–∏—Ä—É–µ–º matplotlib –≥—Ä–∞—Ñ–∏–∫ PCA scatter plot
print("–°–æ–∑–¥–∞–µ–º –∏ –ª–æ–≥–∏—Ä—É–µ–º matplotlib –≥—Ä–∞—Ñ–∏–∫ PCA scatter plot")
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap="viridis", alpha=0.7)
plt.colorbar(scatter)
plt.title("PCA Scatter Plot (2 Components)")
plt.xlabel("First Principal Component")
plt.ylabel("Second Principal Component")

logger.report_matplotlib_figure(
    title="Dataset Visualization",
    series="PCA Scatter Plot",
    figure=plt,
)

# –°–æ–∑–¥–∞–µ–º –∏ –ª–æ–≥–∏—Ä—É–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É
print("–°–æ–∑–¥–∞–µ–º –∏ –ª–æ–≥–∏—Ä—É–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É")
correlation_matrix = df.corr()
fig, ax = plt.subplots(figsize=(10, 8))
im = ax.imshow(correlation_matrix.values, cmap="coolwarm", aspect="auto")
plt.colorbar(im)
ax.set_xticks(range(len(correlation_matrix.columns)))
ax.set_yticks(range(len(correlation_matrix.columns)))
ax.set_xticklabels(correlation_matrix.columns, rotation=45, ha="right")
ax.set_yticklabels(correlation_matrix.columns)
plt.title("Correlation Matrix")
plt.tight_layout()

logger.report_matplotlib_figure(
    title="Dataset Visualization",
    series="Correlation Matrix",
    figure=plt,
)

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –º–æ–¥–µ–ª–∏
hyperparams = {
    "poly_degree_range": list(range(1, 5)),
    "random_state": 2,
    "C": 1.0,
    "max_iter": 100,
}
task.connect(hyperparams)

###############################
##### Preprocessing stage #####
###############################

print("–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö...")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
train_class_counts = Counter(y_train)
test_class_counts = Counter(y_test)

# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
fig, ax = plt.subplots()
ax.bar(list(train_class_counts.keys()), list(train_class_counts.values()))
ax.set_title("Train Class Distribution")
ax.set_xlabel("Class")
ax.set_ylabel("Count")
logger.report_matplotlib_figure(
    title="Preprocessing Visualization",
    series="Train Class Distribution",
    figure=plt,
)

fig, ax = plt.subplots()
ax.bar(list(test_class_counts.keys()), list(test_class_counts.values()))
ax.set_title("Test Class Distribution")
ax.set_xlabel("Class")
ax.set_ylabel("Count")
logger.report_matplotlib_figure(
    title="Preprocessing Visualization",
    series="Test Class Distribution",
    figure=plt,
)

#######################################
##### Hyperparameter tuning stage #####
#######################################

print("–ù–∞—á–∏–Ω–∞–µ–º –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")

poly_degree_range = hyperparams["poly_degree_range"]
train_accuracies = []
val_accuracies = []

for degree in poly_degree_range:
    model = Pipeline(
        [
            ("poly", PolynomialFeatures(degree=degree)),
            (
                "logistic",
                LogisticRegression(
                    random_state=hyperparams["random_state"],
                    C=hyperparams["C"],
                    max_iter=hyperparams["max_iter"],
                ),
            ),
        ]
    )
    model.fit(X_train_scaled, y_train)

    y_train_pred = model.predict(X_train_scaled)
    y_test_pred = model.predict(X_test_scaled)

    train_acc = accuracy_score(y_train, y_train_pred)
    val_acc = accuracy_score(y_test, y_test_pred)

    val_precision = float(precision_score(y_test, y_test_pred))
    val_recall = float(recall_score(y_test, y_test_pred))
    val_f1 = float(f1_score(y_test, y_test_pred))
    train_precision = float(precision_score(y_train, y_train_pred))
    train_recall = float(recall_score(y_train, y_train_pred))
    train_f1 = float(f1_score(y_train, y_train_pred))

    train_accuracies.append(train_acc)
    val_accuracies.append(val_acc)

    logger.report_scalar(
        title="Accuracy",
        series="train",
        value=float(train_acc),
        iteration=degree,
    )
    logger.report_scalar(
        title="Accuracy",
        series="validation",
        value=float(val_acc),
        iteration=degree,
    )
    logger.report_scalar(
        title="Precision",
        series="train",
        value=train_precision,
        iteration=degree,
    )
    logger.report_scalar(
        title="Precision",
        series="validation",
        value=val_precision,
        iteration=degree,
    )
    logger.report_scalar(
        title="Recall", series="train", value=train_recall, iteration=degree
    )
    logger.report_scalar(
        title="Recall", series="validation", value=val_recall, iteration=degree
    )
    logger.report_scalar(
        title="F1 Score", series="train", value=train_f1, iteration=degree
    )
    logger.report_scalar(
        title="F1 Score", series="validation", value=val_f1, iteration=degree
    )

    print(
        f"polynomial_degree={degree}, train_acc={train_acc:.4f}, val_acc={val_acc:.4f}"
    )

best_val_accuracy = max(val_accuracies)
best_poly_degree = poly_degree_range[val_accuracies.index(best_val_accuracy)]

final_accuracy = best_val_accuracy
logger.report_single_value(name="final_accuracy", value=final_accuracy)

best_hyperparams = {
    "best_poly_degree": best_poly_degree,
    "random_state": hyperparams["random_state"],
    "C": hyperparams["C"],
    "max_iter": hyperparams["max_iter"],
}

best_hyperparams_df = pd.DataFrame(
    list(best_hyperparams.items()), columns=["Hyperparameter", "Value"]
)
logger.report_table(
    title="Best Hyperparameters",
    series="Tuned Values",
    iteration=0,
    table_plot=best_hyperparams_df,
)

print("–°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∏ logger.report_table")
results_df = pd.DataFrame(
    {
        "polynomial_degree": poly_degree_range,
        "train_accuracy": train_accuracies,
        "validation_accuracy": val_accuracies,
    }
)
logger.report_table(
    title="Training Results",
    series="Results",
    iteration=0,
    table_plot=results_df,
)

######################################
##### Final model training stage #####
######################################

print("–û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –ª—É—á—à–∏–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...")
logger.report_text("Training final model with best hyperparameters...")

final_model = Pipeline(
    [
        ("poly", PolynomialFeatures(degree=best_poly_degree)),
        (
            "logistic",
            LogisticRegression(
                random_state=hyperparams["random_state"],
                C=hyperparams["C"],
                max_iter=hyperparams["max_iter"],
            ),
        ),
    ]
)
final_model.fit(X_train_scaled, y_train)

y_pred_proba = final_model.predict_proba(X_test_scaled)[:, 1]
y_pred = final_model.predict(X_test_scaled)

##################################
##### Model evaluation stage #####
##################################

print("–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏...")

print("–í—ã—á–∏—Å–ª—è–µ–º ROC curve –∏ —Å—Ç—Ä–æ–∏–º —á–µ—Ä–µ–∑ plotly")
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

fig = go.Figure()
fig.add_trace(
    go.Scatter(
        x=fpr, y=tpr, mode="lines", name=f"ROC Curve (AUC = {roc_auc:.4f})"
    )
)
fig.add_trace(
    go.Scatter(
        x=[0, 1],
        y=[0, 1],
        mode="lines",
        name="Random Classifier",
        line=dict(dash="dash"),
    )
)
fig.update_layout(
    title="ROC Curve (Plotly)",
    xaxis_title="False Positive Rate",
    yaxis_title="True Positive Rate",
    xaxis=dict(range=[0, 1]),
    yaxis=dict(range=[0, 1]),
)

logger.report_plotly(title="Training Results", series="ROC Curve", figure=fig)

print("–í—ã—á–∏—Å–ª—è–µ–º confusion matrix...")
cm = confusion_matrix(y_test, y_pred)

logger.report_confusion_matrix(
    title="Confusion Matrix",
    series="Validation",
    iteration=0,
    matrix=cm,
    xaxis="Predicted",
    yaxis="Actual",
)

val_precision = float(precision_score(y_test, y_pred))
val_recall = float(recall_score(y_test, y_pred))
val_f1 = float(f1_score(y_test, y_pred))

logger.report_single_value(name="precision", value=val_precision)
logger.report_single_value(name="recall", value=val_recall)
logger.report_single_value(name="f1_score", value=val_f1)

print(f"Precision: {val_precision:.4f}")
print(f"Recall: {val_recall:.4f}")
print(f"F1-score: {val_f1:.4f}")

print("–õ–æ–≥–≥–∏—Ä—É–µ–º —á–∞—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
predictions_df = pd.DataFrame(
    {
        "true_label": y_test,
        "predicted_label": y_pred,
        "prediction_proba": y_pred_proba,
    }
)
logger.report_table(
    title="Sample Predictions",
    series="Debug Samples",
    iteration=0,
    table_plot=predictions_df.head(20),
)

##############################
##### Model saving stage #####
##############################

print("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")

model_path = "models/polynomial_with_dataset.pkl"
os.makedirs(os.path.dirname(model_path), exist_ok=True)
joblib.dump(final_model, model_path, compress=True)

task.close()
print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {final_accuracy:.4f}")
print("–ú–µ—Ç—Ä–∏–∫–∏ –∏ –≥—Ä–∞—Ñ–∏–∫–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –≤ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ ClearML")
print(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç: {dataset.name} (ID –≤–µ—Ä—Å–∏–∏: {dataset.id})")
